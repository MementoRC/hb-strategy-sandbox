name: CI

on:
  push:
    branches: [ main, development ]
  pull_request:
    branches: [ main, development ]

env:
  FORCE_COLOR: 1

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  # Lint and format check
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.1
      with:
        pixi-version: v0.20.1
    
    - name: Run linting
      run: pixi run ci-check
    
    - name: Run type checking
      run: pixi run ci-hints

  # Unit tests with separation for better isolation
  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          # Ubuntu: Full Python version coverage (primary development platform)
          - os: ubuntu-latest
            python-version: "3.10"
            pixi-environment: "py310"
          - os: ubuntu-latest
            python-version: "3.11"
            pixi-environment: "py311"
          - os: ubuntu-latest
            python-version: "3.12"
            pixi-environment: "py312"
          
          # macOS: Python 3.12+ only (aligned with Hummingbot requirements)
          - os: macos-latest
            python-version: "3.12"
            pixi-environment: "py312"
          
          # Windows: Core versions only
          - os: windows-latest
            python-version: "3.10"
            pixi-environment: "py310"
          - os: windows-latest
            python-version: "3.12"
            pixi-environment: "py312"

    defaults:
      run:
        shell: bash

    steps:
      - uses: actions/checkout@v4

      - name: Install Pixi
        uses: prefix-dev/setup-pixi@v0.8.1
        with:
          environments: ${{ matrix.pixi-environment }}

      - name: Install project dependencies (using Pixi)
        run: |
          pixi install -e ${{ matrix.pixi-environment }}
          echo "Python version check:"
          pixi run -e ${{ matrix.pixi-environment }} python --version

      - name: Run unit tests with coverage
        run: |
          # Run unit tests first to avoid URL patching pollution from integration tests
          # Exclude benchmark tests from CI blocking - they're for performance monitoring only
          pixi run -e ${{ matrix.pixi-environment }} pytest tests -m "not integration and not benchmark" --cov=strategy_sandbox --cov-report=xml:coverage.xml

      - name: Run integration tests separately
        run: |
          # Run integration tests separately to prevent URL patching pollution
          # Exclude benchmark tests from CI blocking - they're for performance monitoring only
          # Allow integration tests to pass if no tests are found (exit code 5)
          pixi run -e ${{ matrix.pixi-environment }} pytest tests -m "integration and not benchmark" --cov=strategy_sandbox --cov-append || [ $? -eq 5 ]

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
        with:
          files: ./coverage.xml
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: MementoRC/hb-strategy-sandbox
          fail_ci_if_error: false

  # Performance benchmarks (non-blocking)
  benchmark:
    name: Performance Benchmarks (Non-blocking)
    runs-on: ubuntu-latest
    needs: [test]
    if: github.event_name == 'pull_request' || github.ref == 'refs/heads/main'
    continue-on-error: true  # Don't block CI if benchmarks fail
    defaults:
      run:
        shell: bash

    steps:
      - uses: actions/checkout@v4

      - name: Install Pixi
        uses: prefix-dev/setup-pixi@v0.8.1
        with:
          environments: default

      - name: Install project dependencies (using Pixi)
        run: |
          pixi install -e default
          pixi run -e default python --version

      - name: Create benchmark results directory
        run: |
          mkdir -p benchmark-results

      - name: Run performance benchmarks
        run: |
          # Run performance benchmark tests for monitoring (non-blocking)
          # These tests measure performance characteristics but don't block CI
          # Capture JSON output for performance trend analysis
          pixi run -e default pytest tests -m "benchmark" -v --tb=short \
            --benchmark-json=benchmark-results/benchmark-results.json \
            --benchmark-sort=mean

      - name: Generate benchmark summary
        if: always()
        run: |
          if [ -f benchmark-results/benchmark-results.json ]; then
            echo "## Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo "Benchmark completed successfully. Results stored as artifacts." >> $GITHUB_STEP_SUMMARY

            # Extract key metrics for summary
            python3 -c "import json; data=json.load(open('benchmark-results/benchmark-results.json')); benchmarks=data.get('benchmarks',[]); print(f'Total benchmarks: {len(benchmarks)}'); print(f'Fastest test: {min(benchmarks,key=lambda x:x[\"stats\"][\"mean\"])[\"name\"]} ({min(benchmarks,key=lambda x:x[\"stats\"][\"mean\"])[\"stats\"][\"mean\"]:.2f}μs)') if benchmarks else None; print(f'Slowest test: {max(benchmarks,key=lambda x:x[\"stats\"][\"mean\"])[\"name\"]} ({max(benchmarks,key=lambda x:x[\"stats\"][\"mean\"])[\"stats\"][\"mean\"]:.2f}μs)') if benchmarks else None" >> $GITHUB_STEP_SUMMARY
          else
            echo "Benchmark results file not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Store benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            benchmark-results/
            tests/performance/
          retention-days: 30
      
      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request' && success()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const benchmarkResults = fs.readFileSync('benchmark-results.json', 'utf8');
              const results = JSON.parse(benchmarkResults);
              
              const comment = `## 📊 Performance Benchmark Results
              
              | Metric | Value | Change |
              |--------|-------|--------|
              | Avg Response Time | ${results.avg_response_time || 'N/A'} | ${results.change_response_time || 'N/A'} |
              | Memory Usage | ${results.memory_usage || 'N/A'} | ${results.change_memory || 'N/A'} |
              | Throughput | ${results.throughput || 'N/A'} ops/sec | ${results.change_throughput || 'N/A'} |
              
              <details>
              <summary>View detailed results</summary>
              
              \`\`\`json
              ${JSON.stringify(results, null, 2)}
              \`\`\`
              </details>
              
              📈 **Non-blocking**: These benchmarks monitor performance trends but don't affect CI status.`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('No benchmark results found or error reading results:', error.message);
            }

  # Build and package verification
  build:
    runs-on: ubuntu-latest
    needs: [lint, test]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.1
      with:
        pixi-version: v0.20.1
    
    - name: Build package
      run: pixi run -e default python -m build
    
    - name: Check package
      run: pixi run -e default python -m twine check dist/*
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist-${{ github.sha }}
        path: dist/
        retention-days: 7

  # Security scanning
  security:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
    
    - name: Run Bandit Security Scan
      run: |
        pip install bandit[toml]
        bandit -r strategy_sandbox -f json -o bandit-report.json || true
        bandit -r strategy_sandbox || true
    
    - name: Run Safety Check
      run: |
        pip install safety
        safety check --json --output safety-report.json || true
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-scan-${{ github.sha }}
        path: |
          bandit-report.json
          safety-report.json
        retention-days: 30

  # Summary job that reports overall workflow status
  ci-status:
    name: CI Status Summary
    runs-on: ubuntu-latest
    needs: [lint, test, build, benchmark, security]
    if: always()
    steps:
      - name: Report CI Status
        run: |
          # Check if all required jobs passed
          if [[ "${{ needs.test.result }}" == "success" && "${{ needs.lint.result }}" == "success" && "${{ needs.build.result }}" == "success" && "${{ needs.security.result }}" == "success" ]]; then
            echo "✅ All CI jobs passed"
            echo "## 🎉 CI Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
            echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
            echo "| Tests | ✅ ${{ needs.test.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Lint | ✅ ${{ needs.lint.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Build | ✅ ${{ needs.build.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Security | ✅ ${{ needs.security.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Benchmark | 📈 ${{ needs.benchmark.result }} (non-blocking) |" >> $GITHUB_STEP_SUMMARY
            exit 0
          else
            echo "❌ Some CI jobs failed"
            echo "## ❌ CI Status: FAILURE" >> $GITHUB_STEP_SUMMARY
            echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
            echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
            echo "| Tests | ${{ needs.test.result == 'success' && '✅' || '❌' }} ${{ needs.test.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Lint | ${{ needs.lint.result == 'success' && '✅' || '❌' }} ${{ needs.lint.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Build | ${{ needs.build.result == 'success' && '✅' || '❌' }} ${{ needs.build.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Security | ${{ needs.security.result == 'success' && '✅' || '❌' }} ${{ needs.security.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Benchmark | 📈 ${{ needs.benchmark.result }} (non-blocking) |" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi